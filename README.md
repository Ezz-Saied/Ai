# ğŸ§  Linear Regression using Stochastic Gradient Descent (SGD)

This project implements **Linear Regression from scratch** using **Stochastic Gradient Descent (SGD)** in Python â€” without using any machine learning libraries.  
It also includes a list of **10 common Machine Learning optimizers** with short explanations in PDF format.

---

## ğŸ“ Project Structure
```
ğŸ“‚ AI-LinearRegression-SGD
â”‚
â”œâ”€â”€ linear_regression.py      # Python code for Linear Regression using SGD
â”œâ”€â”€ dataset.csv               # Input dataset (features and target)
â”œâ”€â”€ ML_Optimizers_List.pdf    # List of 10 ML optimizers with descriptions
â”œâ”€â”€ README.md                 # Project documentation
```

---

## ğŸš€ Project Overview

### ğŸ”¹ Objective
- Implement a **Linear Regression model** that learns to predict target values using **SGD** (Stochastic Gradient Descent).  
- Avoid using pre-built ML libraries such as `sklearn`.

### ğŸ”¹ Key Concepts
- **Linear Regression** finds the best-fit line by minimizing Mean Squared Error (MSE).
- **SGD** updates model parameters after each random sample for faster convergence on large datasets.

### ğŸ”¹ Equation
\[
y = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + Î¸â‚ƒxâ‚ƒ + ... + Î¸â‚™xâ‚™
\]

---

## âš™ï¸ How It Works

1. **Load Dataset** from `.csv` file using pandas.
2. **Normalize Features** for stable convergence.
3. **Initialize Parameters (Î¸)** randomly.
4. **Iteratively Update Parameters** using:
   \[
   Î¸ = Î¸ - Î± Ã— âˆ‡J(Î¸)
   \]
   where `Î±` = learning rate and `âˆ‡J(Î¸)` = gradient of loss.
5. **Output Learned Parameters** and an example prediction.

---

## ğŸ§© Example Output
```
Learned Parameters (theta): [ 1.10  1.32 -0.64 -0.33 ]
Example Prediction: 149.07
```

---

## ğŸ“Š Optional Visualizations
Add these plots to visualize model performance:
- **Loss vs. Epoch** â€“ to show convergence.
- **Predicted vs. Actual** â€“ to show prediction accuracy.

---

## ğŸ“˜ Files Included

### ğŸ“„ `ML_Optimizers_List.pdf`
Contains short one-line explanations of:
1. Gradient Descent  
2. Stochastic Gradient Descent  
3. Mini-Batch GD  
4. Momentum  
5. Nesterov  
6. Adagrad  
7. RMSProp  
8. Adam  
9. Adadelta  
10. Nadam  

---

## ğŸ’¡ Requirements
Install the required libraries:
```bash
pip install numpy pandas matplotlib
```

Run the script:
```bash
python3 linear_regression.py
```

---

## ğŸ‘¨â€ğŸ’» Author
**Ezzeldin Said**  
AI & Machine Learning Student  

---

## ğŸ License
This project is for educational purposes.  
Feel free to modify or extend it for your learning.

---
